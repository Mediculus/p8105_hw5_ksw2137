---
title: "P8105 Data Science I - Homework 5"
author: "Kevin S.W.   UNI: ksw2137"
date: "10/31/2019"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width = 10, 
                      fig.align = "center",
                      results = "asis"
                      )

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr")
invisible(lapply(Packages, library, character.only = TRUE))



# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0.5, size = 12),
                  plot.subtitle = element_text(hjust = 0.5, size = 8),
                  plot.caption = element_text(hjust = 0, size = 7))
          )

```

# Problem 1

Dataframe setup using the codes given by the class.

```{r problem_setup}

# load tidyverse (although already loaded prior)
library(tidyverse)                                   

# setting seed for reproducibility
set.seed(10)

# loads native data in R and applies NA randomly
iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

```

It is a `r nrow(iris_with_missing)` x `r ncol(iris_with_missing)` matrix with `r sum(is.na(iris_with_missing))` `NA`; `r sum(is.na(iris_with_missing))/ncol(iris_with_missing)` per column. After setting this up, below we have a function that modifies a vector by:

* Filling missing numeric variables with the mean of non-missing values within the column
* Filling missing character variables with character of choice (defaults "virginica")

```{r function_soln}

# replace numeric with average, character with input of choice
avgchar_na_replace <- function(input_vec, char_replace = "virginica") { # function that takes in df and char input 
  if (!is.numeric(input_vec) & !is.character(input_vec)) {              # quick check if input = char/numeric
    stop("Argument should only be numeric or character vector")         # error message
  } else if (is.character(input_vec)) {                                 # if character, replace with char input
    replace_na(input_vec, char_replace)
  } else if (is.numeric(input_vec)) {                                   # if numeric, return a mean of the vector
    round(
      replace_na(                                                       # with 1 decimal
        input_vec, mean(input_vec, na.rm = TRUE)), 1)
  }
}

new_iris <-                                                             # a dataframe that stores the results
  map2(.x = iris_with_missing,                                          # map function that utilizes function above
       .y = "virginica", 
       ~avgchar_na_replace(input_vec = .x, char_replace = .y)) %>% 
  as_tibble()

```

The `new_iris` variable stores new dataframe after applying `avgchar_na_replace` function with `purrr::map2` (`.x` for dataframe and `.y` for character input). If a column is numeric, it will automatically input average of column. Example: `r round(mean(pull(iris_with_missing, Sepal.Length), na.rm = TRUE), 1)`, which is the average of available numbers in `Sepal.Length` column, replaces `NA` in that column. `skimr::skim(new_iris)` reveals no missing variables.

# Problem 2 
We have 20 files that contains 8-weeks longitudinal study result files for 10 subjects under experiment and control arm. First, we need to consolidate this into 1 dataframe.

```{r data_read}

# stores file names inside specified path and turn it into r x 1 tibble
files <- list.files("./data") %>% 
  tibble::enframe(name = NULL) %>%
  rename("file_name" = value)

# maps read_csv to iterate reading all the files based on files above
# and make a listcol out of the read contents, then unnested to get a proper tibble
exp_df <- files %>% 
  mutate(
    weekly_data = map(file_name, ~read_csv(str_c("./data/", .x)))
  ) %>% 
  unnest()
  
```

Now that we have our dataframe, we need to clean it up.

```{r data_clean}
# cleaning the tibble
clean_exp_df <- exp_df %>% 
  janitor::clean_names() %>% 
  separate(col = file_name,                                                     # separate the file name
           into = c("group", "subj_id"), "_") %>% 
  pivot_longer(week_1:week_8,                                                   # turn into long form
               names_to = "week",
               values_to = "observation",
               names_prefix = "week_") %>%                                      # remove common character
  mutate(
    subj_id = as_factor(as.numeric(str_replace(subj_id, "\\.csv", ""))),        # remove file extension, factorize
    group = as_factor(recode(group, "con" = "Control", "exp" = "Experiment")),  # rename group variable, factorize
    week = as_factor(week)                                                      # factorize
    )  

```

Our clean dataframe is a `r nrow(clean_exp_df)` x `r ncol(clean_exp_df)` matrix with `r ncol(clean_exp_df)` variables; `group` (control/experiment), `subj_id` (subject "number"), `week` (week 1, 2,..., 8), and `observation` (recorded data). 

Now that it's tidied, we could start exploring patterns and such. Below is a spaghetti plot that traces the data based on subject and their group.

```{r spaghetti_all, fig.width = 8}
# spaghetti plot of each subject by control/experiment
# requires plotting twice to separate control/experiment

ggplot(clean_exp_df %>%                                                        # plot for experiment group
         filter(group == "Experiment")) +
  geom_point(aes(x = week, y = observation, shape = group),                    # point plot for experiment
             size = 2.5, alpha = 0.8) +
  geom_line(aes(x = week, y = observation, color = subj_id, group = subj_id),  # line color by subject
            size = 0.8, alpha = 0.8) +
  
  # another set of geom_point/_line for control group. Alpha is differentiated to aid discernment
  geom_point(data = clean_exp_df %>%                                           # point plot for control 
               filter(group == "Control"),
             aes(x = week, y = observation, shape = group), 
             size = 2.5, alpha = 0.5) +
  geom_line(data = clean_exp_df %>%                                            # line color by subject
              filter(group == "Control"), 
            aes(x = week, y = observation, color = subj_id, group = subj_id), 
            size = 0.8, alpha = 0.5) +
  guides(color = guide_legend(override.aes = list(size = 5))) + # resize the color code for legends
  labs(x = "Week",
       y = "Observed Measurement",
       caption = "Figure 1: Spaghetti plot of 10 subjects over 8 weeks under experimental and control group",
       color = "Subject ID",
       shape = "Group"
       ) +
  theme(legend.position = "right")

```

As we can see above however, this plot is very messy because there are too many subjects in the plot; even adding shapes doesn't help too much. For greater clarity, we could facet this below

```{r spaghetti_facet}

# better "spaghetti" plot
clean_exp_df %>%                                                # base for facet graph
  ggplot(aes(x = week, y = observation, color = subj_id)) +     # give color by subjects
  geom_point(size = 2) + 
  geom_line(aes(group = subj_id), alpha = 0.7, size = 0.6) +
  facet_grid(. ~ group) +                                       # facet by group
  guides(color = guide_legend(override.aes = list(size = 5))) + # resize the color code for legends
  labs(x = "Week",
       y = "Observed Measurement",
       caption = "Figure 2: Faceted spaghetti plot of 10 subjects over 8 weeks by group",
       color = "Subject ID")

```

Now that it's split between control and experiment, it's much less cluttered and now we could see that the experiment group's observed data increases as the week goes by for all subjects while it stays relatively flat in the control group. 

# Problem 3

When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a simple linear regression.

First set the following design elements:

Fix n=30
Fix xi1 as draws from a standard Normal distribution
Fix β0=2
Fix σ2=50
Set β1=0. Generate 10000 datasets from the model
yi=β0+β1xi1+ϵi
with ϵi∼N[0,σ2]. For each dataset, save β̂ 1 and the p-value arising from a test of H:β1=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of lm.

Repeat the above for β1={1,2,3,4,5,6}, and complete the following:

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of β2 on the x axis. Describe the association between effect size and power.
Make a plot showing the average estimate of β̂ 1 on the y axis and the true value of β1 on the x axis. Make a second plot (or overlay on the first) the average estimate of β̂ 1 only in samples for which the null was rejected on the y axis and the true value of β1 on the x axis. Is the sample average of β̂ 1 across tests for which the null is rejected approximately equal to the true value of β1? Why or why no